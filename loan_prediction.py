# -*- coding: utf-8 -*-
"""Loan prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gECu5Iz5p12PBwdgTJw-RLJ2soufNRp7
"""

import numpy as np
import pandas as pd
import matplotlib
import seaborn as sns
import plotly.express as px
from sklearn import datasets, linear_model, preprocessing
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

train = pd.read_csv('/content/drive/MyDrive/769/univ.ai/Training Data.csv')
test = pd.read_csv('/content/drive/MyDrive/769/univ.ai/Test Data.csv')

print("Rows, columns: " + str(train.shape))
train.head()

print("Rows, columns: " + str(test.shape))
test.head()

print(train.isna().sum())
print(test.isna().sum())

train.dtypes

print(np.sum(train['risk_flag']))
print(len(train['risk_flag']))

sns.distplot(train['age'])

train['car_ownership'] = train['car_ownership'].map({'no':0, 'yes':1}).astype(np.int)
test['car_ownership'] = test['car_ownership'].map({'no':0, 'yes':1}).astype(np.int)
train['married'] = train['married'].map({'single':0, 'married':1}).astype(np.int)
test['married'] = test['married'].map({'single':0, 'married':1}).astype(np.int)

def convert_age(my_list):
  age = my_list
  for i in range(len(age)):
    if (age[i]>=20 and age[i]<40):
      age[i] =1
    elif (age[i]>=40 and age[i]<60):
      age[i]=2
    elif (age[i]>=60 and age[i]<80):
      age[i]=3   
  my_list = age

convert_age(train['age'])
convert_age(test['age'])

from sklearn.preprocessing import LabelEncoder
category= ['profession','house_ownership','state','city'] 
encoder= LabelEncoder()
for i in category:
  train[i] = encoder.fit_transform(train[i])
  test[i]=encoder.fit_transform(test[i]) 
test.head()

#train = pd.get_dummies(train,prefix=['prof'],columns=['profession'])
#test = pd.get_dummies(test,prefix=['prof'],columns=['profession'])
#train = pd.get_dummies(train,prefix=['house'],columns=['house_ownership'])
#test = pd.get_dummies(test,prefix=['house'],columns=['house_ownership'])
#train = pd.get_dummies(train,prefix=['state'],columns=['state'])
#test = pd.get_dummies(test,prefix=['state'],columns=['state'])

train_final=train.drop(['city','Id','profession'],axis=1)
test_final = test.drop(['city','profession'],axis=1)
train_final.head()

xtrain=train_final.drop(["risk_flag"],axis=1)
ytrain=train_final["risk_flag"]

X=xtrain
y =ytrain

# setting up testing and training sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
#model = LogisticRegression(max_iter=10000,tol=100000 )
model= DecisionTreeClassifier()
#model = RandomForestClassifier(n_estimators=1500)
model.fit(X_train,y_train)

from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
y_pred = model.predict(X_test)
print("Training accuracy :", model.score(X_train, y_train))
print("Testing accuracy :", model.score(X_test, y_test))
cf_matrix = confusion_matrix(y_test, y_pred)

group_names = ['True_Neg','False_Pos','False_Neg','True_Pos']
group_counts = ['{0:0.0f}'.format(value) for value in
                cf_matrix.flatten()]
group_percentages = ['{0:.2%}'.format(value) for value in
                     cf_matrix.flatten()/np.sum(cf_matrix)]
labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]
labels = np.asarray(labels).reshape(2,2)
sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')

x_final=test_final.drop('id',axis=1)
output=pd.DataFrame(test_final,columns=['id'])
y_final =model.predict(x_final)
print(y_final)
print(np.unique(y_final))

output['risk_flag']=y_final
print(np.sum(y_final))

output.head()

output.to_csv('/content/drive/MyDrive/769/univ.ai/my_submission5.csv', index=False)